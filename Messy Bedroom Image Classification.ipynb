{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada notebook ini akan membuat model CNN untuk mengklasifikasikan foto kamar yang rapi atau berantakan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mendefinisikan nama direktori untuk data latih dan data validasi.\n",
    "train_dir = \"C:/Users/wiart/Documents/Belajar Machine Learning/Messy Bedroom Image Classification/images/train\"\n",
    "validation_dir = \"C:/Users/wiart/Documents/Belajar Machine Learning/Messy Bedroom Image Classification/images/val\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clean', 'messy']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREPROCESSING DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah selanjutnya, kita akan menerapkan ImageDataGenerator untuk data latih dan data validasi. ImageDataGenerator merupakan sebuah fungsi yang sangat berguna untuk mempersiapkan data latih dan data validasi. Dataset akan dilakukan proses augmentasi gambar, dimana \n",
    "sebuah teknik yang dapat digunakan untuk memperbanyak data latih dengan cara menduplikasi gambar yang telah ada dengan menambahkan variasi tertentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dataGen = ImageDataGenerator(\n",
    "                    rescale = 1./255,\n",
    "                    rotation_range = 20,\n",
    "                    horizontal_flip = True,\n",
    "                    shear_range = 0.2,\n",
    "                    fill_mode = 'nearest')\n",
    "\n",
    "test_dataGen = ImageDataGenerator(\n",
    "                    rescale = 1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, siapkan data latih dan validasi dari kumpulan data gambar yang di-load dalam memori melalui fungsi flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 192 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_dataGen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = (150,150), # mengubah resolusi gambar menjadi 150x150\n",
    "    batch_size = 4,\n",
    "    class_mode = 'binary') # karena merupakan masalah klasifikasi 2 kelas, maka gunakan class_mode = binary\n",
    "\n",
    "validation_generator = test_dataGen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size = (150,150), # mengubah resolusi gambar menjadi 150x150\n",
    "    batch_size = 4,\n",
    "    class_mode = 'binary') # karena merupakan masalah klasifikasi 2 kelas, maka gunakan class_mode = binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MEMBUAT MODEL CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah data siap, selanjutnya adalah membangun model Convolutional Neural Network (CNN). Pembuatan model CNN pada keras mirip dengan pembuatan model Multi Layer Perceptron (MLP). Perbedaannya terdapat pada empat lapis layer konvolusi dan max pooling. Pada model CNN, proses klasifikasi gambar hanya berfokus pada atribut-atribut unik yang membedakan tiap kategori. Sehingga, teknik ini dinilai lebih optimal dibandingkan hanya menggunakan model MLP yang membedakan tiap kategori dengan melihat keseluruhan piksel-piksel pada gambar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(512, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usai membuat model, gunakan fungsi summary() untuk melihat summary dari arsitektur model yang telah kita buat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 15, 15, 512)       590336    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 7, 7, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               12845568  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,529,665\n",
      "Trainable params: 13,529,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan hasil summary di atas, model yang kita buat terdiri dari empat lapis Convolutional dan MaxPoling layer, sebuah flatten layer, serta dua buah dense layer. Ingatlah bahwa dense layer terakhir merupakan output layer. Pada kasus klasifikasi biner, output model merupakan angka tunggal antara 0 dan 1. Sehingga, kita set dense layer terakhir = 1. Sementara itu, kolom “Param #” berisi informasi mengenai jumlah parameter pada tiap layer.\n",
    "\n",
    "Selanjutnya, kolom “Output Shape” berisi informasi ukuran output yang dihasilkan tiap layer. Jika diperhatikan, ukuran input gambar yang telah didefinisikan sebelumnya adalah sebesar (150, 150). Tapi pada convolutional layer pertama, setiap satu input gambar akan menghasilkan ukuran output (148, 148) sebanyak 32 gambar. Ukuran tersebut berkurang karena kita menggunakan filter dengan ukuran (3, 3) dengan jumlah filter sebanyak 32 filter. Sehingga, tiap satu input gambar akan menghasilkan 32 gambar baru dengan ukuran (148, 148). \n",
    "\n",
    "Kemudian, resolusi tiap gambar akan diperkecil dengan tetap mempertahankan informasi pada gambar menggunakan MaxPoling layer yang berukuran (2, 2). Hal ini  akan menghasilkan ukuran output gambar sebesar (74, 74). Nah, proses tersebut juga berlaku untuk Convolutional dan MaxPoling layer yang lain. \n",
    "\n",
    "Berikutnya, pada flatten layer. Output dari MaxPoling layer terakhir yang terdiri dari 512 gambar dengan ukuran (7, 7) akan diubah ke dalam bentuk array 1D (tensor 1D). Hal ini  akan menghasilkan output berukuran (25088). \n",
    "\n",
    "Nah, output tersebut kemudian masuk ke dalam dense layer pertama yang memiliki 512 neuron. Sehingga, akan menghasilkan output dengan ukuran (512). Selanjutnya, output ini akan masuk pada dense layer kedua yang memiliki 1 neuron sehingga akan menghasilkan output dengan ukuran (1). Output dari layer terakhir inilah yang digunakan sebagai hasil akhir model untuk kasus klasifikasi biner.\n",
    "\n",
    "Setelah membuat arsitektur model CNN, tahap selanjutnya adalah melakukan compile model tersebut menggunakan fungsi compile(). Loss function yang digunakan pada kasus klasifikasi biner adalah \"binary_crossentropy\". Selain itu, optimizer yang digunakan  pada kasus ini adalah \"Adam optimizer\". Adam optimizer dipilih karena mudah diterapkan, lebih efisien secara komputasi dan kebutuhan memori yang lebih kecil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL FITTING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada proses ini, kita memasukkan data latih pada jaringan Neural Network yang telah kita buat sebelumnya. \n",
    "\n",
    "Hal yang harus didefinisikan pada tahap ini adalah loss function dan optimizer. Kemudian, mulai proses pelatihan model dengan memanggil fungsi fit(). \n",
    "\n",
    "Dengan menggunakan ImageDataGenerator, parameter tidak perlu dimasukan gambar dan labelnya. Image data generator secara otomatis melabeli gambar sesuai dengan direktorinya. Sebagai contoh,  sebuah gambar yang terdapat di direktori clean, akan diberi label “clean” oleh ImageDataGenerator secara otomatis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 - 11s - loss: 0.7523 - accuracy: 0.5200 - val_loss: 0.6924 - val_accuracy: 0.5000 - 11s/epoch - 427ms/step\n",
      "Epoch 2/20\n",
      "25/25 - 9s - loss: 0.6952 - accuracy: 0.4900 - val_loss: 0.6917 - val_accuracy: 0.7000 - 9s/epoch - 360ms/step\n",
      "Epoch 3/20\n",
      "25/25 - 9s - loss: 0.6713 - accuracy: 0.6200 - val_loss: 0.6937 - val_accuracy: 0.5000 - 9s/epoch - 373ms/step\n",
      "Epoch 4/20\n",
      "25/25 - 9s - loss: 0.7211 - accuracy: 0.5400 - val_loss: 0.6942 - val_accuracy: 0.5000 - 9s/epoch - 366ms/step\n",
      "Epoch 5/20\n",
      "25/25 - 9s - loss: 0.7056 - accuracy: 0.5400 - val_loss: 0.6907 - val_accuracy: 0.5000 - 9s/epoch - 365ms/step\n",
      "Epoch 6/20\n",
      "25/25 - 9s - loss: 0.6958 - accuracy: 0.4700 - val_loss: 0.6931 - val_accuracy: 0.5000 - 9s/epoch - 375ms/step\n",
      "Epoch 7/20\n",
      "25/25 - 10s - loss: 0.6934 - accuracy: 0.4000 - val_loss: 0.6931 - val_accuracy: 0.5000 - 10s/epoch - 391ms/step\n",
      "Epoch 8/20\n",
      "25/25 - 10s - loss: 0.6934 - accuracy: 0.5400 - val_loss: 0.6929 - val_accuracy: 0.5000 - 10s/epoch - 411ms/step\n",
      "Epoch 9/20\n",
      "25/25 - 10s - loss: 0.6928 - accuracy: 0.5200 - val_loss: 0.6914 - val_accuracy: 0.5000 - 10s/epoch - 419ms/step\n",
      "Epoch 10/20\n",
      "25/25 - 10s - loss: 0.7224 - accuracy: 0.4700 - val_loss: 0.6648 - val_accuracy: 0.6500 - 10s/epoch - 417ms/step\n",
      "Epoch 11/20\n",
      "25/25 - 10s - loss: 0.6951 - accuracy: 0.4400 - val_loss: 0.6932 - val_accuracy: 0.5000 - 10s/epoch - 380ms/step\n",
      "Epoch 12/20\n",
      "25/25 - 10s - loss: 0.6935 - accuracy: 0.4500 - val_loss: 0.6931 - val_accuracy: 0.5000 - 10s/epoch - 412ms/step\n",
      "Epoch 13/20\n",
      "25/25 - 10s - loss: 0.6932 - accuracy: 0.5100 - val_loss: 0.6931 - val_accuracy: 0.5000 - 10s/epoch - 404ms/step\n",
      "Epoch 14/20\n",
      "25/25 - 10s - loss: 0.6931 - accuracy: 0.5600 - val_loss: 0.6931 - val_accuracy: 0.5000 - 10s/epoch - 387ms/step\n",
      "Epoch 15/20\n",
      "25/25 - 10s - loss: 0.6930 - accuracy: 0.5300 - val_loss: 0.6932 - val_accuracy: 0.5000 - 10s/epoch - 382ms/step\n",
      "Epoch 16/20\n",
      "25/25 - 10s - loss: 0.6926 - accuracy: 0.5600 - val_loss: 0.6932 - val_accuracy: 0.5000 - 10s/epoch - 401ms/step\n",
      "Epoch 17/20\n",
      "25/25 - 11s - loss: 0.6921 - accuracy: 0.5700 - val_loss: 0.6932 - val_accuracy: 0.5000 - 11s/epoch - 429ms/step\n",
      "Epoch 18/20\n",
      "25/25 - 12s - loss: 0.6936 - accuracy: 0.4800 - val_loss: 0.6932 - val_accuracy: 0.5000 - 12s/epoch - 480ms/step\n",
      "Epoch 19/20\n",
      "25/25 - 10s - loss: 0.6940 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000 - 10s/epoch - 401ms/step\n",
      "Epoch 20/20\n",
      "25/25 - 9s - loss: 0.6935 - accuracy: 0.4800 - val_loss: 0.6932 - val_accuracy: 0.5000 - 9s/epoch - 375ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d15b55e100>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=25, # berapa batch yang akan dieksekusi pada setiap epoch\n",
    "    epochs=20, # tambahan epochs jika akurasi model belum optimal\n",
    "    validation_data=validation_generator, # menampilkan akurasi pengujian data validasi\n",
    "    validation_steps=5, # berapa batch yang akan dieksekusi pada setiap epoch\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4a9168551437448198d4f2fec74653a97ed6f61c1486e4fe18d03bbf1e51fe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
